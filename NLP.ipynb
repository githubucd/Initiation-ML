{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement du langage naturel (NLP pour Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Avant de commencer avec les transformers, voyons rapidement ce qu'est le traitement du langage naturel et pourquoi il est important.</p>\n",
    "\n",
    "## I. Le NLP, qu'est-ce que c'est ?\n",
    "\n",
    "<p>Le traitement du langage naturel est un domaine à l'intersection du machine learning et de la linguistique se concentrant sur la compréhension de tout ce qui est lié à la langue humaine. L'objectif des tâches de NLP est non seulement de comprendre individuellement chaque mot, mais aussi de comprendre le contexte associé à l'utilisation de ces mots.</p>\n",
    "\n",
    "<p>La liste suivante regroupe les tâches de NLP les plus courantes, avec pour chacune quelques exemples :</p>\n",
    "<ul>\n",
    "    <li><b>Classification de phrases entières</b> : analyser le sentiment d'un avis, détecter si un email est un spam, déterminer si une phrase est grammaticalement correcte, déterminer si deux phrases sont logiquement reliées ou non, etc.</li>\n",
    "    <li><b>Classification de chaque mot d'une phrase</b> : identifier les composants grammaticaux d'une phrase (nom, verbe, adjectif), identifier les entités nommées (personne, lieu, organisation), etc.</li>\n",
    "    <li><b>Génération de texte</b> : compléter le début d'un texte avec un texte généré automatiquement, remplacer les mots manquants ou masqués dans un texte, etc.</li>\n",
    "    <li><b>Extraction d'une réponse à partir d'un texte</b> : étant donné une question et un contexte, le but est d'extraire la réponse à la question en fonction des informations fournies par le contexte, etc.</li>\n",
    "    <li><b>Génération de nouvelles phrases à partir d'un texte</b> : traduire un texte dans une autre langue, faire le résumé d'un texte, etc.</li>\n",
    "</ul>\n",
    "<p>Le traitement du langage naturel ne se limite pas qu'à la compréhension du texte. Il s'intéresse aussi aux problèmes complexes de reconnaissance de la parole et de vision par ordinateur tels que la génération d'une transcription à partir d'un échantillon audio ou la description d'une image.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autres domaines d'application\n",
    "<ul>\n",
    "    <li><b>Classification de texte</b> : cela consiste à attribuer un ensemble de catégories prédéfinies à un texte donné. Les classificateurs de texte peuvent être utilisés pour organiser, structurer et catégoriser à ensemble de textes.</li>\n",
    "    <li><b>Reconnaissance de caractères</b> :  Cela permet d’extraire, à partir de la reconnaissance des caractères, les principales informations des reçus, des factures, des chèques, des documents de facturation légaux, etc.</li>\n",
    "    <li><b>Correction automatique</b> : la plupart des éditeurs de texte sont aujourd’hui muni d’un correcteur orthographique qui permet de vérifier si le texte contient des fautes d’orthographe.</li>\n",
    "<li><b>Résumé automatique</b> : les méthodes NLP sont également utilisées pour produire des résumés courts, précis et fluides d’un document texte plus long.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Les grands principes du NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Le NLP est généralement composé de deux à trois grandes étapes:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Pré-traitement : une étape qui cherche à standardiser du texte afin de rendre son usage plus facile</li>\n",
    "<li>Représentation du texte comme un vecteur : Cette étape peut être effectuée via des techniques de sac de mots (Bag of Words) ou Term Frequency-Inverse Document Frequency (Tf-IdF). On peut également apprendre des représentations vectorielles (embedding) par apprentissage profond.</li>\n",
    "<li>Classification, trouver la phrase la plus similaire….\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. La phase de prétraitement : du texte aux données\n",
    "\n",
    "<p>Supposons que nous voulions déterminer si un mail est un spam ou non, uniquement à partir de son contenu. À cette fin, il est indispensable de transformer les données brutes (le texte du mail) en des données exploitables. </p>\n",
    "\n",
    "<p>Parmi les principales étapes, on retrouve :</p>\n",
    "<ul>\n",
    "    <li><b>Nettoyage</b> : Variable selon la source des données. Cette étape commence en général par une phase de nettoyage qui varie selon la source des données. Par exemple, dans le cas de données issues de réseaux sociaux ou forums, il peut être nécessaire de supprimer les smileys, emojis, urls et autres spécificités (hashtags, mentions…).</li>\n",
    "    <li><b>Normalisation des données</b> : </li>\n",
    "    <ul>\n",
    "        <li><b>Tokenisation :</b>, ou découpage du texte en plusieurs pièces appelés tokens. \n",
    "Par exemple, dans la phrase : « Vous trouverez en pièce jointe le document en question » ; « Vous », « trouverez », « en pièce jointe », « le document », « en question » en sont des tokens.</li>\n",
    "        <li><b>Stemming</b> : un même mot peut se retrouver sous différentes formes en fonction du genre (masculin féminin), du nombre (singulier, pluriel), la personne (moi, toi, eux…) etc. Le stemming désigne généralement le processus qui ne consiste qu'à conserver que la racine du mot.\n",
    "    Exemple : « trouverez » -> « trouv »</li>\n",
    "        <li><b>Lemmatisation</b> : cela consiste à réaliser la même tâche que le stemming, mais en utilisant un vocabulaire et une analyse fine de la construction des mots. La lemmatisation permet donc de supprimer uniquement les terminaisons inflexibles et donc à isoler la forme canonique du mot, connue sous le nom de lemme. Exemple : « trouvez » -> trouver</li>\n",
    "        <li><b>Autres opérations</b> : suppression des chiffres, ponctuation, symboles et stopwords, passage en minuscule.</li>\n",
    "    </ul>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Représentation du texte comme un vecteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Afin de pouvoir appliquer les méthodes de Machine Learning aux problèmes relatifs au langage naturel, il est indispensable de transformer les données textuelles en données numériques.</p>\n",
    "<p>Il existe plusieurs approches dont les principales sont les suivantes :</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Term-Frequency (TF)</b> : cette méthode consiste à compter le nombre d’occurrences des tokens présents dans le corpus pour chaque texte. Chaque texte est alors représenté par un vecteur d’occurrences. On parle généralement de Bag-Of-Word, ou sac de mots en français.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/TF.webp\" width=\"500\"><br>\n",
    "<center>Représentation des vecteurs issues de la méthode Term-Frequency (TF)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Néanmoins, cette approche présente un inconvénient majeur : certains mots sont par nature plus utilisés que d’autres, ce qui peut conduire le modèle à des résultats erronés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><b>Term Frequency-Inverse Document Frequency (TF-IDF)</b> : cette méthode consiste à compter le nombre d’occurrences des tokens présents dans le corpus pour chaque texte, que l’on divise ensuite par le nombre d’occurrences total de ces même tokens dans tout le corpus.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le terme x présent dans le document y, on peut définir son poids par la relation suivante :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/TFIDF.webp\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Où :\n",
    "<ul>\n",
    "    <li>tƒx,y est la fréquence du terme x dans y ;</li>\n",
    "    <li>dƒx est le nombre de documents contenant x ;</li>\n",
    "    <li>N est le total de documents.</li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Cette approche permet donc d’obtenir pour chaque texte une représentation vectorielle qui comporte des vecteurs de poids et non plus d’occurrences.</p>\n",
    "\n",
    "<p>L’efficacité de ces deux méthodes (TF et TF-IDF) diffère selon le cas d’application. Toutefois, elles présentent deux principales limites :</p>\n",
    "<ul>\n",
    "<li>Plus le vocabulaire du corpus est riche, plus la taille des vecteurs est grande, ce qui peut représenter un problème pour les modèles d’apprentissage utilisées dans l’étape suivante.</li>\n",
    "<li>Le comptage d’occurrence des mots ne permet pas de rendre compte de leur agencement (sequencement) et donc du sens des phrases. En effet, la position du mot dans le vocabulaire était arbitraire, elle ne permet pas de décrire une éventuelle relation entre eux</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><b>Word Embedding</b> : est une autre approche qui permet de remédier à les problèmes de <b>TF</b> et <b>TF-IDF</b>. \n",
    "    Elle consiste à construire des vecteurs de taille fixe qui prennent en compte le contexte dans lequel se trouvent les mots. <p>Ainsi, deux mots présents dans des contextes similaires auront des vecteurs plus proches (en terme de distance vectorielle).</p>\n",
    "    <p>Cela permet alors de capturer à la fois similarités sémantiques, syntaxiques ou thématiques des mots.</p></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. La phase d'apprentissage : des données au modèle\n",
    "<p>De manière globale, on peut distinguer 3 principales approches NLP : les méthodes basées sur des règles, modèles classiques de Machine Learning et modèles de Deep Learning.</p>\n",
    "<ul>\n",
    "    \n",
    "<li><b>Méthodes basées sur des règles</b> : Les méthodes fondées sur des règles reposent en grande partie sur l’élaboration de règles spécifiques à un domaine (par exemple, les expressions régulières). Elles peuvent être utilisées pour résoudre des problèmes simples tels que l’extraction de données structurées à partir de données non structurées (par exemple, les pages web).<br>\n",
    "Dans le cas de la détection de spams, cela pourrait consister à considérer comme e-mails indésirables, ceux qui comportent des buzz words tels que « promotion », « offre limitée », etc.\n",
    "Néanmoins, ces méthodes simples peuvent être rapidement dépassées par la complexité du langage naturel et s’avérer être inefficace.</li>\n",
    "    \n",
    "<li><b>Modèles classiques de Machine Learning</b> : Les approches classiques d’apprentissage automatique peuvent être utilisées pour résoudre des problèmes plus difficiles. Contrairement aux méthodes fondées sur des règles prédéfinies, elles reposent sur des méthodes qui portent réellement sur la compréhension du langage. Elles exploitent les données obtenues à partir des textes bruts prétraités via une des méthodes décrites en haut par exemple. Elles mettent généralement en œuvre un modèle statistique d’apprentissage automatique tels que ceux de Naive Bayes, de Régression Logistique, etc.</li>\n",
    "    \n",
    "<li><b>Modèles de Deep Learning</b> : L’utilisation de modèles d’apprentissage en profondeur pour les problématiques NLP fait l’objet de nombreuses recherches actuellement. </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Les modèles de <b>deep learning</b> se généralisent encore mieux que les approches classiques d’apprentissage car ils nécessitent une phase de prétraitement du texte moins sophistiquée : les couches de neurones peuvent être perçues comme des extracteurs automatiques de features.</p>\n",
    "\n",
    "<p>Cela permet alors de construire des modèles de bout en bout avec peu de prétraitement des données. En dehors de la partie feature engineering, les capacités d’apprentissage des algorithmes de Deep Learning sont généralement plus puissantes que celles de Machine Learning classique, ce qui permet d’obtenir de meilleurs scores sur différentes tâches complexes de NLP dures telles que la traduction. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. NLP avec Python\n",
    "\n",
    "<p>Le langage Python (en) est couramment utilisé pour faire du NLP. Il existe plusieurs librairies permettant de travailler avec le langage naturel. Les plus connues et utilisées sont Gensim (en), NLTK (en) et plus récemment SpaCy (en).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/frameworks.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Pendant longtemps, NLTK (Natural Language ToolKit) a été la librairie Python standard pour le NLP. Elle regroupe des algorithmes de classifications, de Part-of-Speech Tagging, de stemming, de tokenisation (en) de mots et de phrases. Elle contient également des corpora de données et permet de faire de la reconnaissance d’entités nommées (NER) et de l’analyse de sentiments. Même s’il y a des mises à jours régulières, la librairie NLTK commence un peu à dater (2001) et montre quelques limites notamment en terme de performance.</p>\n",
    "\n",
    "<p>Une librairie plus récente (2015) semble avoir pris le relais de NLTK, il s’agit de SpaCy. Cette librairie écrite en Python et Cython regroupe les mêmes types d’outils que NLTK : tokenisation, POS-tagging, NER, analyse de sentiments (toujours en développement), lemmatisation. Elle possède également des vecteurs de mots pré-entraînés et des modèles statistiques dans plusieurs langues (anglais, allemand, français, espagnol etc.).</p>\n",
    "\n",
    "<p>Enfin la troisième librairie mentionnée est un peu différente des deux autres. En effet, Gensim est une librairie spécialisée dans le Topic Modeling. Elle implémente plusieurs algorithmes statistiques de Topic Modeling (en) (Latent Dirichlet Allocation ou LDA, Latent Semantic Indexing ou LSI, Hierarchical Dirichlet Process (en) ou HDP) et permet également de faire du Word Embedding.</p>\n",
    "\n",
    "<p>En plus de ces trois librairies, d’autres peuvent être utiles lorsque l’on veut faire du NLP. On peut citer la librairie de visualisation PyLDAvis (en) permettant de visualiser plus facilement les résultats de Topic Modeling. Une autre librairie importante est la librairie de Machine Learning très connue, Scikit-Learn. Elle permet de faire de la tokenisation, de l’extraction de features (en) et implémente de nombreux algorithmes de classification. Enfin, on peut parler de Keras (en). C’est une API écrite en Python qui permet de construire et d’entraîner des modèles de réseaux de neurones. Ces modèles peuvent notamment être utilisés pour faire de la classification de textes. Keras permet aussi faire du Word-Embedding.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
