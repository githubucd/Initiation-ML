{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK : Natural Language ToolKit\n",
    "\n",
    "Email : <a href='mailto:madani.a@ucd.ac.ma'>madani.a@ucd.ac.ma</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est-ce que c'est NLTK ?\n",
    "<p>\n",
    "Natural Language Toolkit (NLTK) est une boîte-à-outil permettant la création de programmes pour l'analyse de texte (TextMining). Cet ensemble a été créé à l'origine par Steven Bird et Edward Loper, en relation avec des cours de linguistique informatique à l'Université de Pennsylvanie en 2001.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travailler avec NLTK\n",
    "<p>\n",
    "La première chose à faire pour utiliser NLTK est de télécharger ce qui se nomme le <strong>NLTK corpora</strong>. On peut télécharger tout le Corpus. C'est vrai que c'est énorme (à peu près 10,9 Go), mais nous ne le ferons qu'une seule fois. Si vous connaissez déjà quel corpus vous utiliserez, inutile de télécharger cet ensemble.\n",
    "</p>\n",
    "<p>\n",
    "Dans votre éditeur Python IDLE, écrivez ceci :\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Pour télécharger tout le corpus\n",
    "nltk.download()\n",
    "# Pour télécharger seulement le corpus de 'stopwords'\n",
    "nltk.download('stopwords')\n",
    "# Pour télécharger seulement le corpus de 'tokenizer'\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "<p>\n",
    "Parfois, nous avons besoin d'éliminer des éléments inutiles afin que les données soient davatange traduisables pour l'ordinateur. En NLP, de telles données (des mots, words) sont qualifiées par <strong>stop words</strong>. Par conséquent, ces mots n'ont aucune signification pour nous, et nous souhaiterions les retirer.\n",
    "</p>\n",
    "<p>\n",
    "La libraire NLTK contient quelques stopwords. Pour les connaître, écrivons ces petits scripts :\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'were', 'ourselves', \"you've\", \"didn't\", 'those', 'couldn', 'how', 'needn', 't', 'shouldn', 'these', 'a', 'by', \"haven't\", 'been', 'was', \"it's\", 'out', 'doesn', 'haven', 'wasn', 'into', \"don't\", 'on', 'she', 'i', 'you', \"should've\", 'such', 'during', 'of', 'ours', 'very', 'why', 'm', 'am', 'off', 'against', \"mightn't\", 'won', 'or', 'they', 'yours', 'ain', 'your', 'weren', 'for', 'than', 'doing', 'that', 'then', 'our', 'between', 'them', 'just', 'again', 'now', 'at', 'does', 'not', 'it', 'through', 'where', 'aren', 'after', 'do', 'he', 'didn', 'up', 'themselves', 'hers', 'who', \"wouldn't\", 'if', 'its', \"weren't\", \"aren't\", 'ma', 'her', 'we', 'should', 'are', 'and', 'before', \"shan't\", 'other', \"needn't\", 'y', \"couldn't\", 'the', 'be', 'had', 'to', 'my', 'myself', 'same', \"you're\", 'about', 'nor', 'from', 'there', 'below', 'himself', 'having', 'because', 'over', 'hadn', \"hadn't\", \"wasn't\", 'any', 'no', 'with', 's', 'him', 'an', \"won't\", 'until', 'theirs', 'under', 'hasn', 'wouldn', 'both', \"that'll\", \"shouldn't\", 'down', 'too', 'will', \"isn't\", 'me', 'isn', 'as', 'but', 'did', 'some', 'in', 'whom', 'll', 'herself', 'here', \"she's\", 'mustn', 'being', 'what', 'when', 'can', 'have', \"hasn't\", 'is', 'above', 'further', 'more', \"mustn't\", 'itself', 'their', 'all', \"you'd\", 're', 'don', 'his', 'so', \"doesn't\", \"you'll\", 'this', 'while', 'd', 'o', 'each', 'own', 'yourselves', 'most', 'shan', 'which', 'yourself', 'few', 'only', 'mightn', 'once', 've', 'has'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mais', 'aient', 'fusses', 'dans', 'serais', 'avions', 't', 'aviez', 'les', 'n', 'leur', 'eue', 'aux', 'fût', 'auriez', 'j', 'que', 'ayante', 'à', 'on', 'pour', 'nos', 'serai', 'serons', 'étante', 'serez', 'ton', 'étais', 'fussions', 'ai', 'ayants', 'suis', 'aurai', 'aurions', 'm', 'eussiez', 'lui', 'avaient', 'ayez', 'seriez', 'étés', 'étant', 'sa', 'étantes', 'étée', 'aie', 'été', 'es', 'ayons', 'ait', 'ont', 'sois', 'fussiez', 'fûtes', 'et', 'mes', 'avec', 'étions', 'est', 'vos', 'eusses', 'fusse', 'qu', 'l', 'mon', 'sont', 'du', 'elle', 'ma', 'êtes', 'auraient', 'ses', 'serait', 'fûmes', 'y', 'un', 'ayantes', 'ayant', 'aurais', 'sera', 'soit', 'eusse', 'moi', 'de', 'auront', 'fut', 'aurons', 'seras', 'était', 'se', 'ta', 'étants', 'eussions', 'eussent', 'ces', 'avons', 'eu', 'avais', 'tes', 'par', 'eûmes', 'en', 'soient', 'avait', 'auras', 's', 'tu', 'même', 'le', 'sur', 'ne', 'son', 'pas', 'au', 'vous', 'seront', 'ce', 'notre', 'me', 'as', 'votre', 'aura', 'eut', 'une', 'nous', 'sommes', 'eûtes', 'seraient', 'qui', 'étaient', 'des', 'eues', 'eût', 'ils', 'la', 'étées', 'te', 'toi', 'aurait', 'fussent', 'je', 'eus', 'soyez', 'aurez', 'd', 'ou', 'c', 'fus', 'furent', 'aies', 'serions', 'il', 'soyons', 'eurent', 'étiez', 'eux', 'avez'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'لكيلا', 'هذان', 'إليكن', 'اللتين', 'بهن', 'بل', 'حبذا', 'ذوا', 'لئن', 'حين', 'ذه', 'ممن', 'لولا', 'فيما', 'يا', 'إليكم', 'إن', 'كلتا', 'سوف', 'ذي', 'لستم', 'قد', 'لكم', 'ذين', 'هيا', 'كل', 'إما', 'إنما', 'ومن', 'كي', 'في', 'التي', 'أما', 'أنتم', 'ذلكم', 'لنا', 'إذا', 'اللذان', 'إي', 'لكي', 'آي', 'فلا', 'ذانك', 'مذ', 'إلى', 'بخ', 'بها', 'ته', 'لك', 'أيها', 'ولو', 'بعض', 'حتى', 'عدا', 'أين', 'عن', 'أينما', 'عسى', 'بهم', 'لكن', 'ليت', 'بعد', 'بمن', 'بين', 'دون', 'إنه', 'كليهما', 'أنت', 'ليستا', 'بنا', 'كأن', 'إليك', 'لم', 'لهن', 'منه', 'فمن', 'فيه', 'إيه', 'غير', 'عليه', 'ليسوا', 'لستن', 'ما', 'وإذا', 'عل', 'لي', 'هاتان', 'عليك', 'ذواتا', 'أم', 'هي', 'منذ', 'والذين', 'والذي', 'حيث', 'بما', 'لدى', 'خلا', 'ولا', 'إذن', 'منها', 'أنتما', 'اللتيا', 'كما', 'ثم', 'أولئك', 'له', 'هن', 'ريث', 'تي', 'كيف', 'هذي', 'بهما', 'بس', 'وإذ', 'مه', 'كأي', 'هناك', 'وإن', 'فإن', 'الذين', 'أكثر', 'هكذا', 'تلكم', 'لسنا', 'ليسا', 'أقل', 'نعم', 'آه', 'ولكن', 'بكن', 'كلاهما', 'وهو', 'شتان', 'حيثما', 'كيت', 'لعل', 'كلا', 'هيهات', 'بيد', 'أن', 'لستما', 'كلما', 'هل', 'ذواتي', 'هاتين', 'لست', 'هذه', 'لكنما', 'لهم', 'عند', 'مع', 'تينك', 'إنا', 'لوما', 'لها', 'بماذا', 'ليست', 'هما', 'ذينك', 'أنتن', 'إلا', 'لما', 'متى', 'هاتي', 'هاهنا', 'فإذا', 'كأين', 'ماذا', 'تين', 'كذا', 'ذان', 'به', 'ليس', 'بك', 'لو', 'كأنما', 'أولاء', 'ذلك', 'هاته', 'أف', 'اللذين', 'إليكما', 'كيفما', 'لن', 'نحن', 'بكم', 'لا', 'أوه', 'هيت', 'هلا', 'إذ', 'هنا', 'أو', 'اللاتي', 'مهما', 'على', 'إذما', 'نحو', 'ذلكما', 'حاشا', 'هم', 'أي', 'كم', 'هذين', 'ذات', 'تلكما', 'هؤلاء', 'فيها', 'سوى', 'لهما', 'لاسيما', 'ها', 'ذاك', 'فيم', 'اللتان', 'الذي', 'كليكما', 'من', 'أنى', 'ذا', 'أنا', 'كذلك', 'هذا', 'اللائي', 'هو', 'ثمة', 'ذلكن', 'آها', 'اللواتي', 'بكما', 'هنالك', 'تلك', 'لكما', 'ذو', 'هاك', 'ألا', 'لسن', 'بلى', 'وما', 'بي', 'عما', 'مما'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('arabic')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing (segmentation du texte)\n",
    "<p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens :\n",
      "['In', 'this', 'tutorial', ',', 'You', \"'re\", 'learning', 'NLTK', '.', 'It', 'is', 'an', 'interesting', 'platform', '.', 'You', \"'re\", 'lucky']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"In this tutorial, You're learning NLTK. It is an interesting platform. You're lucky\"\n",
    "\n",
    "words = word_tokenize(text, language='english')\n",
    "print(\"Liste des tokens :\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'platform', 'is', 'an', 'NLTK', 'You', 'interesting', 'lucky', 'tutorial', '.', 'learning', \"'re\", 'this', 'In', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"In this tutorial, You're learning NLTK. It is an interesting platform. You're lucky\"\n",
    "\n",
    "words = set(word_tokenize(text, language='english'))\n",
    " \n",
    "new_sentence = []\n",
    " \n",
    "for word in words:\n",
    "    new_sentence.append(word)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elimination des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens sans StopWords:\n",
      "['tutorial', ',', \"'re\", 'learning', 'NLTK', '.', 'interesting', 'platform', '.', \"'re\", 'lucky']\n"
     ]
    }
   ],
   "source": [
    "new_sentence = []\n",
    " \n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        new_sentence.append(word)\n",
    "print(\"Liste des tokens sans StopWords:\") \n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est la première phrase.\n",
      "Puis j'en écris une seconde.\n",
      "pour finir en voilà une troisième sans mettre de majuscule\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "\n",
    "text_fr = \"Ceci est la première phrase. Puis j'en écris une seconde. pour finir en voilà une troisième sans mettre de majuscule\"\n",
    " \n",
    "# We use the segmentation function on the text\n",
    "sentences = sent_tokenize(text_fr, language = 'french')\n",
    " \n",
    "# We print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "En linguistique, le stemming, la racinisation ou désuffixation est un procédé de transformation des mots en leur radical ou racine. La racine d’un mot correspond à la partie du mot restante une fois que l’on a supprimé son (ses) préfixe(s) et suffixe(s), afin d'obtenir son radical\n",
    "</p>\n",
    "<p>\n",
    "Deux principales familles d'algorithme de stemming sont présentes dans la littérature : orientés algorithmes et ceux utilisant un dictionnaire.\n",
    "<ul>\n",
    "<li><strong>Orienté algorithmes</strong> plus rapide et permet d'extraire des racines de mots inconnus.\n",
    "<li><strong>L'approche par dictionnaire</strong> quant à elle ne fait pas d'erreur sur les mots connus, mais en produit sur ceux qu'elle ne liste pas. Elle est aussi plus lente, et nécessite malgré tout la suppression de suffixes avant d'aller chercher la racine correspondante dans le dictionnaire.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cistem',\n",
       " 'ISRIStemmer',\n",
       " 'LancasterStemmer',\n",
       " 'PorterStemmer',\n",
       " 'RSLPStemmer',\n",
       " 'RegexpStemmer',\n",
       " 'SnowballStemmer',\n",
       " 'StemmerI',\n",
       " 'WordNetLemmatizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'cistem',\n",
       " 'isri',\n",
       " 'lancaster',\n",
       " 'porter',\n",
       " 'regexp',\n",
       " 'rslp',\n",
       " 'snowball',\n",
       " 'util',\n",
       " 'wordnet']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Information sur nktl.stem : différents méthodes de stemming\n",
    "import nltk.stem\n",
    "dir(nltk.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de stemming : porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'dog', 'plane', 'flower', 'fli', 'fri', 'fri', 'week', 'plant', 'run', 'throttl']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import porter\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "porter = porter.PorterStemmer()\n",
    "p_words=[]\n",
    "for word in input_words:\n",
    "    p_words.append(porter.stem(word))\n",
    "print(p_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de steming : lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movy', 'dog', 'plan', 'flow', 'fli', 'fri', 'fry', 'week', 'plant', 'run', 'throttle']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import lancaster\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "lancaster = lancaster.LancasterStemmer()\n",
    "l_words=[]\n",
    "for word in input_words:\n",
    "    l_words.append(lancaster.stem(word))\n",
    "print(l_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de steming : snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'dog', 'plane', 'flower', 'fli', 'fri', 'fri', 'week', 'plant', 'run', 'throttl']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import snowball\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "snowball = snowball.EnglishStemmer()\n",
    "s_words=[]\n",
    "for word in input_words:\n",
    "    s_words.append(snowball.stem(word))\n",
    "print(s_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "<p>\n",
    "La lemmatisation désigne l'analyse lexicale du contenu d'un texte regroupant les mots d'une même famille. Chacun des mots d'un contenu se trouve ainsi réduit en une entité appelée lemme (forme canonique). La lemmatisation regroupe les différentes formes que peut revêtir un mot, soit : le nom, le pluriel, le verbe à l'infinitif, etc.\n",
    "</p>\n",
    "<p>\n",
    "La lemmatisation d'une forme d'un mot consiste à en prendre sa forme canonique. Celle-ci est définie comme suit :\n",
    "<ul>\n",
    "<li>pour un verbe : ce verbe à l'infinitif,\n",
    "<li>pour les autres mots : le mot au masculin singulier.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'dog', 'plane', 'flower', 'fly', 'fry', 'fry', 'week', 'planted', 'running', 'throttle', 'good', 'better', 'best', 'movie', 'child']\n"
     ]
    }
   ],
   "source": [
    "# N'oublier pas de télacharger nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks',\n",
    "'planted','running','throttle','good','better','best','movies', 'children']\n",
    "\n",
    "wordnet_lemm = WordNetLemmatizer()\n",
    "words =[]\n",
    "for w in input_words:\n",
    "    words.append(WordNetLemmatizer().lemmatize(w))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminer les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('English'))\n",
    "text = \"In this tutorial, I'm learning NLTK. It is an interesting platform. Good luck\".lower()\n",
    "\n",
    "words = word_tokenize(text, language='english')\n",
    " \n",
    "tokens = []\n",
    "result=[]\n",
    " \n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        tokens.append(word)\n",
    "for word in tokens:\n",
    "    if word not in string.punctuation:\n",
    "        result.append(word)\n",
    "print(tokens)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
